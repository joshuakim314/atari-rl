{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "csc413-final-josh",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCpYmeN1lOcz"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rFaTvFab6Kz"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYzXFNp7oLiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd91255-4c39-476c-a9c3-c5a789f645bb"
      },
      "source": [
        "!pip install gym-retro\n",
        "# !pip install tensorflow==1.15"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-retro) (0.15.7)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from gym-retro) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-retro) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHB7DA_WHkEm",
        "outputId": "3231916c-4651-4366-b860-a6d17a24e268"
      },
      "source": [
        "!pip install git+https://github.com/openai/baselines"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/baselines\n",
            "  Cloning https://github.com/openai/baselines to /tmp/pip-req-build-xixlplo1\n",
            "  Running command git clone -q https://github.com/openai/baselines /tmp/pip-req-build-xixlplo1\n",
            "Requirement already satisfied (use --upgrade to upgrade): baselines==0.1.6 from git+https://github.com/openai/baselines in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (0.15.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.0.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (1.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from baselines==0.1.6) (4.1.2.30)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.16.0)\n",
            "Building wheels for collected packages: baselines\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-cp37-none-any.whl size=220664 sha256=df810967a34b9b286e643cd7c181ac912cff9ad5c7794cba7e9943d5a1f60389\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vykx5xt2/wheels/d8/55/79/53870462a2ec11a80a4edf582cd45f173da693fe317190ee29\n",
            "Successfully built baselines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSzSEwKdN7yu",
        "outputId": "24f2e8c4-1c88-42b2-c65e-9e388211bea7"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.15.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9fFfA-gb8oC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f99b813-10f1-4c84-9b9f-46f4256abf47"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfTPg6uZckCm"
      },
      "source": [
        "## Import openAI gym and define the functions used to show the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KvZYSl6RrzD"
      },
      "source": [
        "import copy\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLl9cs6ncAf0"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  # env = Monitor(env, './video')\n",
        "  return env"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m6cSZ7PtlNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4047ef-e26a-4b40-8984-198476a28797"
      },
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "\n",
        "#check out the pacman action space!\n",
        "print(env.action_space)\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMcGALcWeWfh"
      },
      "source": [
        "from torch import randint\n",
        "from time import sleep\n",
        "\n",
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "reward_arr = []\n",
        "episode_count = 20\n",
        "for i in tqdm(range(episode_count)):\n",
        "    obs, done, rew = env.reset(), False, 0\n",
        "    env.render()\n",
        "    while not done:\n",
        "        A = randint(0, env.action_space.n, (1,))\n",
        "        obs, reward, done, info = env.step(A.item())\n",
        "        rew += reward\n",
        "        sleep(0.01)\n",
        "    reward_arr.append(rew)\n",
        "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL8sFLSyuMS0"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B54NHM8kRXW-"
      },
      "source": [
        "from gym import envs\n",
        "print(envs.registry.all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmzbPJBGc8mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804d58b9-998a-430d-bb2e-30c92a69d4a8"
      },
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "from stable_baselines.common.policies import ActorCriticPolicy, register_policy, nature_cnn\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines import A2C\n",
        "\n",
        "# Custom MLP policy of three layers of size 128 each for the actor and 2 layers of 32 for the critic,\n",
        "# with a nature_cnn feature extractor\n",
        "class CustomPolicy(ActorCriticPolicy):\n",
        "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **kwargs):\n",
        "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse, scale=True)\n",
        "\n",
        "        with tf.variable_scope(\"model\", reuse=reuse):\n",
        "            activ = tf.nn.relu\n",
        "\n",
        "            extracted_features = nature_cnn(self.processed_obs, **kwargs)\n",
        "            extracted_features = tf.layers.flatten(extracted_features)\n",
        "\n",
        "            pi_h = extracted_features\n",
        "            for i, layer_size in enumerate([128, 128, 128]):\n",
        "                pi_h = activ(tf.layers.dense(pi_h, layer_size, name='pi_fc' + str(i)))\n",
        "            pi_latent = pi_h\n",
        "\n",
        "            vf_h = extracted_features\n",
        "            for i, layer_size in enumerate([32, 32]):\n",
        "                vf_h = activ(tf.layers.dense(vf_h, layer_size, name='vf_fc' + str(i)))\n",
        "            value_fn = tf.layers.dense(vf_h, 1, name='vf')\n",
        "            vf_latent = vf_h\n",
        "\n",
        "            self._proba_distribution, self._policy, self.q_value = \\\n",
        "                self.pdtype.proba_distribution_from_latent(pi_latent, vf_latent, init_scale=0.01)\n",
        "\n",
        "        self._value_fn = value_fn\n",
        "        self._setup_init()\n",
        "\n",
        "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
        "        if deterministic:\n",
        "            action, value, neglogp = self.sess.run([self.deterministic_action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        else:\n",
        "            action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n",
        "                                                   {self.obs_ph: obs})\n",
        "        return action, value, self.initial_state, neglogp\n",
        "\n",
        "    def proba_step(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.policy_proba, {self.obs_ph: obs})\n",
        "\n",
        "    def value(self, obs, state=None, mask=None):\n",
        "        return self.sess.run(self.value_flat, {self.obs_ph: obs})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51iULzZQhT3r"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from stable_baselines import DDPG\n",
        "from stable_baselines.ddpg.policies import LnMlpPolicy\n",
        "from stable_baselines import results_plotter\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n",
        "from stable_baselines.common.noise import AdaptiveParamNoiseSpec\n",
        "from stable_baselines.common.callbacks import BaseCallback\n",
        "from stable_baselines.common.schedules import LinearSchedule\n",
        "\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, 'best_model')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "        with open('./rewards.npy', 'wb') as f:\n",
        "            np.save(f, np.array([]))\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        rewards = np.load('./rewards.npy')\n",
        "        x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "        if len(x) > 0: rewards = np.append(rewards, y[-1])\n",
        "\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "          # Retrieve training reward\n",
        "          # x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
        "          if len(x) > 0:\n",
        "              # Mean training reward over the last 100 episodes\n",
        "              mean_reward = np.mean(y[-100:])\n",
        "              if self.verbose > 0:\n",
        "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
        "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
        "\n",
        "              # New best model, you could save the agent here\n",
        "              if mean_reward > self.best_mean_reward:\n",
        "                  self.best_mean_reward = mean_reward\n",
        "                  # Example for saving best model\n",
        "                  if self.verbose > 0:\n",
        "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
        "                  self.model.save(self.save_path)\n",
        "\n",
        "        with open('./rewards.npy', 'wb') as f:\n",
        "            np.save(f, rewards)\n",
        "\n",
        "        return True\n",
        "\n",
        "# Create log dir\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwvVGRycuQVd"
      },
      "source": [
        "from stable_baselines.common.policies import MlpPolicy, CnnPolicy\n",
        "from stable_baselines.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines.common import set_global_seeds, make_vec_env\n",
        "from stable_baselines import ACKTR, PPO2\n",
        "\n",
        "def make_env(env_id, rank=0, seed=0, log_dir=None, wrapper_class=None, env_kwargs=None):\n",
        "    \"\"\"\n",
        "    Helper function to multiprocess training\n",
        "    and log the progress.\n",
        "    :param env_id: (str)\n",
        "    :param rank: (int)\n",
        "    :param seed: (int)\n",
        "    :param log_dir: (str)\n",
        "    :param wrapper: (type) a subclass of gym.Wrapper to wrap the original\n",
        "                    env with\n",
        "    :param env_kwargs: (Dict[str, Any]) Optional keyword argument to pass to the env constructor\n",
        "    \"\"\"\n",
        "    if log_dir is not None:\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    if env_kwargs is None:\n",
        "        env_kwargs = {}\n",
        "\n",
        "    def _init():\n",
        "        set_global_seeds(seed + rank)\n",
        "        env = gym.make(env_id, **env_kwargs)\n",
        "\n",
        "        if wrapper_class:\n",
        "            env = wrapper_class(env)\n",
        "\n",
        "        env.seed(seed + rank)\n",
        "        log_file = os.path.join(log_dir, str(rank)) if log_dir is not None else None\n",
        "        env = Monitor(env, log_file)\n",
        "        return env\n",
        "\n",
        "    return _init\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env_id = \"Skiing-v0\"\n",
        "    algo_type = 'ppo'\n",
        "    num_cpu = 4  # Number of processes to use\n",
        "    # Create the vectorized environment\n",
        "    env = SubprocVecEnv([make_env(env_id, i, log_dir=log_dir) for i in range(num_cpu)])\n",
        "\n",
        "    # model = PPO2(CustomPolicy, env, verbose=0)\n",
        "    policy = CustomPolicy\n",
        "    time_steps = 1e5\n",
        "    sched_LR = LinearSchedule(time_steps, 0.00025, 0)\n",
        "    model = PPO2(policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=sched_LR.value, vf_coef=1, max_grad_norm=0.5, lam=0.95, nminibatches=128, noptepochs=3, cliprange=0.2, cliprange_vf=None, verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None, full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None)\n",
        "\n",
        "    # Create the callback: check every N steps\n",
        "    callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir)\n",
        "    # Train the agent\n",
        "    model.learn(total_timesteps=int(time_steps), callback=callback)\n",
        "    model.save(algo_type+\"_\"+env_id)\n",
        "\n",
        "    rewards = np.load('./rewards.npy')\n",
        "    with open('./rewards_' + algo_type + '_' + env_id + '.npy', 'wb') as f:\n",
        "            np.save(f, rewards)\n",
        "\n",
        "    results_plotter.plot_results([log_dir], time_steps, results_plotter.X_TIMESTEPS, algo_type + \": \" + env_id)\n",
        "    plt.show()\n",
        "    env.close()\n",
        "    del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWIsQXY2jQJP"
      },
      "source": [
        "y = np.load('./rewards_' + algo_type + '_' + env_id + '.npy')\n",
        "plt.plot(np.arange(y.shape[0]), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb7BQTDttbZs"
      },
      "source": [
        "env.close()\n",
        "env_id = 'Riverraid-v0'\n",
        "algo_type = 'ppo'\n",
        "model = PPO2.load(algo_type+\"_\"+env_id)\n",
        "# model = PPO2.load('./tmp/best_model.zip')\n",
        "env = wrap_env(gym.make(env_id))\n",
        "reward_arr = []\n",
        "episode_count = 5\n",
        "for i in tqdm(range(episode_count)):\n",
        "    obs, done, rew = env.reset(), False, 0\n",
        "    env.render()\n",
        "    while not done:\n",
        "        action, _states = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        rew += reward\n",
        "    reward_arr.append(rew)\n",
        "print(\"average reward per episode :\", sum(reward_arr) / len(reward_arr))\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVrwryxy-dtU"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reward = np.load('rewards_ppo_clip_Riverraid-v0.npy')\n",
        "df = pd.DataFrame(np.array(reward), columns = ['Rewards'])\n",
        "df['Timesteps'] = df.index\n",
        "df['Rolling_Avg_Rewards'] = df.Rewards.rolling(5000).mean()\n",
        "\n",
        "df = pd.DataFrame(np.array(reward_mod), columns = ['Rewards'])\n",
        "df['Timesteps'] = df.index\n",
        "df['Rolling_Avg_Rewards'] = df.Rewards.rolling(5000).mean()\n",
        "\n",
        "plt.plot(df['Timesteps'], df['Rewards'], alpha=0.5, label='Return')\n",
        "plt.plot(df['Timesteps'], df['Rolling_Avg_Rewards'], label='Moving Mean Return')\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Rewards')\n",
        "plt.title('Riverraid-v0: PPO-Clip')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rEOW07J29VW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79pemcgb3EqW"
      },
      "source": [
        "def mish(input):\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self): super().__init__()\n",
        "    def forward(self, input): return mish(input)\n",
        "\n",
        "# helper function to convert numpy arrays to tensors\n",
        "def t(x): return torch.from_numpy(x).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uur90Jf_3F1P"
      },
      "source": [
        "# Actor module, categorical actions only\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            activation(),\n",
        "            nn.Linear(64, 32),\n",
        "            activation(),\n",
        "            nn.Linear(32, n_actions),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.model(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsZICPk73Okb"
      },
      "source": [
        "# Critic module\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            activation(),\n",
        "            nn.Linear(64, 32),\n",
        "            activation(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.model(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKvdv-Ue3Vij"
      },
      "source": [
        "env = wrap_env(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "# config\n",
        "state_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "actor = Actor(state_dim, n_actions, activation=Mish)\n",
        "critic = Critic(state_dim, activation=Mish)\n",
        "adam_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
        "adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYgL2jHI43w_"
      },
      "source": [
        "def clip_grad_norm_(module, max_grad_norm):\n",
        "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)\n",
        "\n",
        "def policy_loss(old_log_prob, log_prob, advantage, eps):\n",
        "    ratio = (log_prob - old_log_prob).exp()\n",
        "    clipped = torch.clamp(ratio, 1-eps, 1+eps)*advantage\n",
        "    \n",
        "    m = torch.min(ratio*advantage, clipped)\n",
        "    return -m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a_50SaB49pZ"
      },
      "source": [
        "episode_rewards = []\n",
        "gamma = 0.98\n",
        "eps = 0.2\n",
        "w = tensorboard.SummaryWriter()\n",
        "s = 0\n",
        "max_grad_norm = 0.5\n",
        "\n",
        "for i in range(5):\n",
        "    prev_prob_act = None\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "\n",
        "    while not done:\n",
        "        s += 1\n",
        "        probs = actor(t(state))\n",
        "        dist = torch.distributions.Categorical(probs=probs)\n",
        "        action = dist.sample()\n",
        "        prob_act = dist.log_prob(action)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action.detach().data.numpy())\n",
        "        advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))\n",
        "        \n",
        "        w.add_scalar(\"loss/advantage\", advantage, global_step=s)\n",
        "        w.add_scalar(\"actions/action_0_prob\", dist.probs[0], global_step=s)\n",
        "        w.add_scalar(\"actions/action_1_prob\", dist.probs[1], global_step=s)\n",
        "        \n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        \n",
        "        if prev_prob_act:\n",
        "            actor_loss = policy_loss(prev_prob_act.detach(), prob_act, advantage.detach(), eps)\n",
        "            w.add_scalar(\"loss/actor_loss\", actor_loss, global_step=s)\n",
        "            adam_actor.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            # clip_grad_norm_(adam_actor, max_grad_norm)\n",
        "            w.add_histogram(\"gradients/actor\",\n",
        "                             torch.cat([p.grad.view(-1) for p in actor.parameters()]), global_step=s)\n",
        "            adam_actor.step()\n",
        "\n",
        "            critic_loss = advantage.pow(2).mean()\n",
        "            w.add_scalar(\"loss/critic_loss\", critic_loss, global_step=s)\n",
        "            adam_critic.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            # clip_grad_norm_(adam_critic, max_grad_norm)\n",
        "            w.add_histogram(\"gradients/critic\",\n",
        "                             torch.cat([p.data.view(-1) for p in critic.parameters()]), global_step=s)\n",
        "            adam_critic.step()\n",
        "        \n",
        "        prev_prob_act = prob_act\n",
        "    \n",
        "    w.add_scalar(\"reward/episode_reward\", total_reward, global_step=i)\n",
        "    episode_rewards.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}